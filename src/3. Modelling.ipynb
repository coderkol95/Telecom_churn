{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business problem:\n",
    "\n",
    "Companies usually have a greater focus on customer acquisition than customer. However, it can cost anywhere between five to twenty five times more to attract a new customer than retain an existing one. Increasing customer retention rates by 5% can increase profits by 25%, according to a research done by Bain & Company.\n",
    "\n",
    "Churn is a metric that measures the no. of customers who stop doing business with a company. Through this metric, most businesses would try to understand the reason behind churn numbers and tackle those factors with reactive action plans.\n",
    "\n",
    "But what if you could identify a customer who is likely to churn and take appropriate steps to prevent it from happening? The reasons that lead customers to the cancellation decision can be numerous, ranging from poor service quality to new competitors entering the market. Usually, there is no single reason, but a combination of factors that result to customer churn.\n",
    "\n",
    "Although the customers have churned, their data is still available. Through machine learning we can sift through this valuable data to discover patterns and understand the combination of different factors which lead to customer churn.\n",
    "\n",
    "Our goal in this project is to identify behavior among customers who are likely to churn. Subsequent to that we need to train a machine learning model to identify these signals from a customer before they churn. Once deployed, our model will identify customers who might churn and alert us to take necessary steps to prevent their churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take input as formatted data:train, validation and test from the data processed python file. \r\n",
    "\r\n",
    "# Use HyperOpt and genetic algorithms for parameter tuning\r\n",
    "\r\n",
    "# Output the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\r\n",
    "#\r\n",
    "#Importing libraries\r\n",
    "#\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer\r\n",
    "from sklearn.model_selection import train_test_split as tts\r\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from imblearn.over_sampling import SMOTE\r\n",
    "from numpy import random\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, recall_score as R\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import warnings\r\n",
    "from sklearn.preprocessing import LabelBinarizer\r\n",
    "from sklearn.feature_selection import chi2\r\n",
    "import pickle\r\n",
    "###############################################################################\r\n",
    "#\r\n",
    "#Notebook options\r\n",
    "#\r\n",
    "pd.options.display.max_columns =100\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "###############################################################################\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"../Data/train.csv\")\r\n",
    "test = pd.read_csv(r\"../Data/test.csv\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.iloc[:,:-1], train.iloc[:,-1]\r\n",
    "X_test, y_test = test.iloc[:,:-1], test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch=pd.DataFrame(chi2(X_train, y_train)).transpose()\r\n",
    "ch.columns=['Chi squared value','p-value']\r\n",
    "ch['p-value'] = ch['p-value'].apply(lambda x: float(x))\r\n",
    "to_drop = ch[ch['p-value']>0.05].index.tolist()\r\n",
    "to_drop = [str(x) for x in to_drop]\r\n",
    "\r\n",
    "X_train.drop(to_drop,axis=1,inplace=True)\r\n",
    "X_test.drop(to_drop,axis=1,inplace=True)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=RandomUnderSampler()\r\n",
    "X_train, y_train = r.fit_resample(X_train, y_train)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=LogisticRegression()\r\n",
    "l.fit(X_train,y_train)\r\n",
    "y_pre=l.predict(X_valid)\r\n",
    "print(roc_auc_score(y_valid,y_pre))\r\n",
    " \r\n",
    "x=GaussianNB()\r\n",
    "x.fit(X_train,y_train)\r\n",
    "y_pre_x=x.predict(X_valid)\r\n",
    "print(roc_auc_score(y_valid,y_pre_x))\r\n",
    "\r\n",
    "s=SVC()\r\n",
    "s.fit(X_train,y_train)\r\n",
    "y_pre_s=s.predict(X_valid)\r\n",
    "print(roc_auc_score(y_valid,y_pre_s))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\r\n",
    "#\r\n",
    "# def build(model,X_train, X_test, y_train, y_test,c,e)\r\n",
    "# \r\n",
    "# Input:\r\n",
    "# model = the model object\r\n",
    "# X_train, X_test = standard scaled train and test features\r\n",
    "# y_train, y_test = response variables in train and test set\r\n",
    "# c = Revenue lost due to churn of a singular customer\r\n",
    "# e = Cost of focusing effort on a single to prevent his/her churn\r\n",
    "#\r\n",
    "# 1. Initialise variables \r\n",
    "# 2. Fit the model\r\n",
    "# 3. r = recall_score\r\n",
    "# 4. p = count of missed positive churn predictions \r\n",
    "# 5. Unravel the confusion matrix\r\n",
    "# 6. Calculate F1 score\r\n",
    "# 7. Calculate ROC score\r\n",
    "# 8. Revenue = (Predicted & actual churn)*c -(count of missed actual positive prediction)*c - (predicted churn)*e\r\n",
    "# 9. Send computed values to business function\r\n",
    "#\r\n",
    "###############################################################################\r\n",
    "\r\n",
    "def build(model,X_train, X_test, y_train, y_test,c,e):  \r\n",
    "\r\n",
    "    rev,p,r=0,0,0\r\n",
    "\r\n",
    "    model.fit(X_train,y_train)\r\n",
    "    y_pr=model.predict(X_test)\r\n",
    "\r\n",
    "    r=round(recall_score(y_test,y_pr)*100,2)\r\n",
    "    p=round((100-r),2)\r\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pr).ravel()\r\n",
    "    f1 = f1_score(y_test,y_pr)\r\n",
    "    roc = roc_auc_score(y_test, y_pr)\r\n",
    "    rev = tp*c -fn*c - (tp+fp)*e\r\n",
    "    return round(r,2), round(p,2), f1, roc,rev  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\r\n",
    "#\r\n",
    "# def compute(df,c,e):\r\n",
    "# \r\n",
    "# Input:\r\n",
    "# df = input dataframe  \r\n",
    "# c = Revenue lost due to churn of a singular customer\r\n",
    "# e = Cost of focusing effort on a single to prevent his/her chur\r\n",
    "#\r\n",
    "# 1. Preprocesses the data\r\n",
    "# 2. Splits the data into training and test set; undersamples the overrepresented class in training set \r\n",
    "# 3. Prepares models and calculates parameters\r\n",
    "# 4. Collates all the data into a mod dataframe\r\n",
    "# 5. Return the comparison database\r\n",
    "#\r\n",
    "#########################################################################################\r\n",
    "\r\n",
    "def compute(X_train,y_train,X_test,y_test, *var):\r\n",
    "    \r\n",
    "    c=var[0]\r\n",
    "    e=var[1]\r\n",
    "    #X,y=preprocess(df)\r\n",
    "    #X_train, X_test, y_train, y_test = \r\n",
    "\r\n",
    "    svm = SVC(kernel='rbf')\r\n",
    "    b=BernoulliNB(alpha=0.6)\r\n",
    "    gbc=GradientBoostingClassifier(learning_rate=0.2, max_depth=1, n_estimators=150)\r\n",
    "    ada=AdaBoostClassifier(learning_rate=0.3, n_estimators=70)\r\n",
    "    lr=LogisticRegression(C=0.001)\r\n",
    "    rf=RandomForestClassifier(max_depth=10,n_estimators=120, class_weight=\"balanced_subsample\", random_state=123456)   \r\n",
    "\r\n",
    "    pred_b,misd_b,f1_b,roc_b,rev_b=build(b,X_train, X_test, y_train, y_test,c,e)\r\n",
    "    pred_knn,misd_knn,f1_knn,roc_knn,rev_knn=build(gbc,X_train, X_test, y_train, y_test,c,e)\r\n",
    "    pred_ada,misd_ada,f1_ada,roc_ada,rev_ada=build(ada,X_train, X_test, y_train, y_test,c,e)\r\n",
    "    pred_lr,misd_lr,f1_lr,roc_lr,rev_lr=build(lr,X_train, X_test, y_train, y_test,c,e)\r\n",
    "    pred_svm,misd_svm,f1_svm,roc_svm,rev_svm=build(svm,X_train, X_test, y_train, y_test,c,e)\r\n",
    "    pred_rf,misd_rf,f1_rf,roc_rf,rev_rf=build(rf,X_train, X_test, y_train, y_test,c,e)\r\n",
    "\r\n",
    "    rev = [rev_svm,rev_b,rev_knn,rev_lr,rev_rf,rev_ada]\r\n",
    "    misd= [misd_svm,misd_b,misd_knn,misd_lr,misd_rf,misd_ada]\r\n",
    "    pred= [pred_svm,pred_b,pred_knn,pred_lr,pred_rf,pred_ada]\r\n",
    "    f1s= [f1_svm,f1_b,f1_knn,f1_lr,f1_rf,f1_ada]\r\n",
    "    rocs= [roc_svm,roc_b,roc_knn,roc_lr,roc_rf,roc_ada]\r\n",
    "    mod = [\"Support Vector Machine\", \"Naive Bayes\",\"Gradient Boosting Classifier\",\"Logistic regression\",\"Random Forest\",\"AdaBoost Classifier\"]\r\n",
    "    model = [svm,b,gbc,lr,rf,ada]\r\n",
    "\r\n",
    "    mod = pd.DataFrame({\"Revenue saved\":rev,\"Predicted(True positive)\":pred,\"Missed(False negative)\":misd,\"F1 score\":f1s, \"ROC_AUC\":rocs,\"Model\":model}, index=mod)\r\n",
    "    mod.sort_values([\"Revenue saved\"], ascending=False,inplace=True)\r\n",
    "    \r\n",
    "    return mod,X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business implication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#\n",
    "# def business(mod,c,e):\n",
    "# Input:\n",
    "# mod = model details dataframe\n",
    "# c = Revenue lost due to churn of a singular customer\n",
    "# e = Cost of focusing effort on a single to prevent his/her chur\n",
    "#  \n",
    "# 1. lost = Total revenue lost due to churn = (Total reponses=1, i.e. churn) * (cost of churn)\n",
    "# 2. best = best model as per revenue\n",
    "# 3. Initialize some variables\n",
    "# 4. Draw >30 samples of size 50% of df\n",
    "# 5. x = Selecting random set of 50% customers\n",
    "# 6. saved_churn = Count of churn predicted by the current sample * cost of churn\n",
    "# 7. cost_of_effort = Total expenditure by focusing effort on random 50% of df\n",
    "# 8. money_that_could_have_been_saved = difference between focusing effort on random 50% of the population and return on it by saving churn\n",
    "# 9. cost_saved_by_model = revenue loss prevented by predicting churn using our model\n",
    "# 10. avg_rev_lost = averages the money that could have been saved over all the 50 samples drawn each time\n",
    "# 11. avg_money_saved = cost saved by our model + money that could have been saved\n",
    "# 12. cost = averages the money that could have been saved over all sample draws\n",
    "# 13. gained_rev = averages the money saved by our model as compared to random sampling over all sample draws\n",
    "# 14. Print summary\n",
    "#\n",
    "#########################################################################################\n",
    "\n",
    "def business(df,y,mod,c,e):\n",
    "\n",
    "    lb=LabelBinarizer()\n",
    "    lost=lb.fit_transform(y).sum()*c \n",
    "\n",
    "    best=mod.head(1)\n",
    "\n",
    "    avg_rev_lost=[]\n",
    "    avg_money_saved=[]\n",
    "\n",
    "    for i in range(1,31):\n",
    "        var_churn=0.5\n",
    "        x = random.randint(df.shape[0], size=(round(df.shape[0]*var_churn)))     \n",
    "        saved_churn = (lb.fit_transform(df.Churn.iloc[x])).sum()*c  \n",
    "        cost_of_effort = x.shape[0]*e\n",
    "        \n",
    "        money_that_could_have_been_saved = cost_of_effort-saved_churn\n",
    "        cost_saved_by_model=best.iloc[0,0]\n",
    "\n",
    "        avg_rev_lost.append(money_that_could_have_been_saved)\n",
    "        avg_money_saved.append(cost_saved_by_model+money_that_could_have_been_saved)\n",
    "\n",
    "    cost = pd.DataFrame(avg_rev_lost).mean()\n",
    "    gained_rev = pd.DataFrame(avg_money_saved).mean()\n",
    "\n",
    "    print(f\"Lost revenue if we do not prevent churn = Rs.{lost} \\n\") \n",
    "    print(f\"Assumed cost of losing a customer: Rs.{c} \\nAssumed cost of effort to prevent churn: Rs.{e} \\n\")\n",
    "    print(f\"Percentage of customers predicted by '{best.index[0]}' who were going to churn: {best.iloc[0,1]}%\")\n",
    "    print(f\"Percentage of customers missed by '{best.index[0]}' who were going to churn: {best.iloc[0,2]}%\")\n",
    "    print(\"Revenue saved by preventing churn with our model as compared to no model = Rs\", best.iloc[0,0])\n",
    "    print(f\"\\n\\nTotal expenditure for preventing churn on random {var_churn*100}% of customers: Rs.{cost_of_effort}\")\n",
    "    print(f\"Extra cost to prevent churn within random {var_churn*100}% of the customers = Rs.{round(cost[0])}\")\n",
    "    print(f\"Our '{best.index[0]}' model saves us Rs.{round(gained_rev[0])} on an average compared to a random selection of 50% customers\")\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\r\n",
    "\r\n",
    "    c = input(\"Enter the revenue lost due to churn of a single customer\")\r\n",
    "    e = input(\"Cost of focusing effort on a customer to prevent his/her churn\")\r\n",
    "    c = 0 if c=='' else int(c)\r\n",
    "    e = 0 if e=='' else int(e)\r\n",
    "    if c>0 and e>0:\r\n",
    "        var = [c,e]\r\n",
    "    else: \r\n",
    "        var=[5000,1500]\r\n",
    "    mod,X,y=compute(X_train,y_train,X_test,y_test,*var)\r\n",
    "    business(df,y,mod,*var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(max_depth=10,n_estimators=120, class_weight=\"balanced_subsample\", random_state=123456)\r\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../deployment/model','wb') as a:\n",
    "    pickle.dump(model,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[1,2]\r\n",
    "q=[str(z) for z in p]\r\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.naive_bayes import BernoulliNB\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.preprocessing import LabelBinarizer\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from numpy import random\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score, precision_score\r\n",
    "from sklearn.feature_selection import chi2\r\n",
    "import pickle\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "\r\n",
    "train = pd.read_csv(r\"../data/train.csv\")\r\n",
    "test = pd.read_csv(r\"../data/test.csv\")\r\n",
    "\r\n",
    "r = RandomUnderSampler()\r\n",
    "\r\n",
    "X_train = train.iloc[:,:-1]\r\n",
    "X_test = test.iloc[:,:-1]\r\n",
    "\r\n",
    "y_train=train.iloc[:,-1]\r\n",
    "y_test=test.iloc[:,-1]\r\n",
    "\r\n",
    "#X_train, y_train = r.fit_resample(X_train, y_train)\r\n",
    "\r\n",
    "ch=pd.DataFrame(chi2(X_train, y_train)).transpose()\r\n",
    "ch.columns=['Chi squared value','p-value']\r\n",
    "ch['p-value'] = ch['p-value'].apply(lambda x: float(x))\r\n",
    "to_drop = ch[ch['p-value']>0.05].index.tolist()\r\n",
    "to_drop=[str(x) for x in to_drop]\r\n",
    "X_train.drop(to_drop,axis=1,inplace=True)\r\n",
    "X_test.drop(to_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\r\n",
    "from hyperopt import hp, fmin, tpe, Trials\r\n",
    "from hyperopt.pyll import scope\r\n",
    "from functools import partial\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def optimize(X,y,params):\r\n",
    "\r\n",
    "    s=StratifiedKFold(n_splits=5)\r\n",
    "    mod = GradientBoostingClassifier(**params)\r\n",
    "    score=cross_val_score(mod,X,y,scoring='accuracy',cv=s).mean()    \r\n",
    "    return -score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space_gb =  {\r\n",
    "        'learning_rate': hp.uniform('lr',0.01,1),\r\n",
    "        'n_estimators': scope.int(hp.quniform('trees',50,1000,1)),\r\n",
    "        'max_depth': scope.int(hp.quniform('deep',1,12,1)),\r\n",
    "        'max_features': hp.uniform('random_feats',0.1,1)\r\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space_lr = {\r\n",
    "    'l1_ratio':hp.uniform('l1',0.1,1),\r\n",
    "    'C': hp.uniform('reg',0.1,2),\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:42<00:00, 32.45s/trial, best loss: -0.7687261328193336]\n",
      "{'deep': 11.0, 'lr': 0.6582840135458902, 'random_feats': 0.7335010971533944, 'trees': 331.0}\n"
     ]
    }
   ],
   "source": [
    "trials=Trials()\r\n",
    "\r\n",
    "optimization_function= partial(optimize,X_train,y_train)\r\n",
    "\r\n",
    "result = fmin(\r\n",
    "    fn=optimization_function,\r\n",
    "    max_evals=10,\r\n",
    "    space=param_space_gb,\r\n",
    "    trials=trials,\r\n",
    "    algo=tpe.suggest\r\n",
    ")\r\n",
    "\r\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Documents\\Codebase\\envs\\machine_L\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5857359635811836, 0.603125, 0.5693215339233039, 0.8062455642299503)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=LogisticRegression(l1_ratio= 0.822518308737693, C=0.5531659031372516)\r\n",
    "g.fit(X_train,y_train)\r\n",
    "yp=g.predict(X_test)\r\n",
    "f1_score(y_test, yp), precision_score(y_test, yp),recall_score(y_test,yp), accuracy_score(y_test,yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6060606060606061, 0.6230529595015576, 0.5899705014749262, 0.815471965933286)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=AdaBoostClassifier()\r\n",
    "g.fit(X_train,y_train)\r\n",
    "yp=g.predict(X_test)\r\n",
    "f1_score(y_test, yp), precision_score(y_test, yp),recall_score(y_test,yp),  accuracy_score(y_test,yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5480314960629922,\n",
       " 0.5878378378378378,\n",
       " 0.5132743362831859,\n",
       " 0.7963094393186657)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=GradientBoostingClassifier(max_depth=11, learning_rate= 0.6582840135458902, max_features= 0.7335010971533944, n_estimators= 331)\r\n",
    "g.fit(X_train,y_train)\r\n",
    "yp=g.predict(X_test)\r\n",
    "f1_score(y_test, yp), precision_score(y_test, yp),recall_score(y_test,yp),accuracy_score(y_test,yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62a490a61034db1e8b6e05e6e999b5629625384177f9d8186b44b788547d5428"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('machine_L': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "25e55770fcf34c599020a25d496c9f3b95f72a4b8e22976c31f9a97ef8dc14b4"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}